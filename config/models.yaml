# Model Configuration
# Defines specific model configurations and parameters

# Embedding models
embeddings:
  models:
    intfloat/e5-large-v2:
      type: "sentence_transformer"
      dimensions: 1024
      max_sequence_length: 512
      pooling: "mean"
      normalization: true
      query_prefix: "query: "
      document_prefix: "passage: "
      device_preference: ["cuda", "cpu"]
      local_path: "./models/embeddings/e5-large-v2"
      use_local_files: true
      memory_requirements:
        gpu: "2GB"
        cpu: "1GB"

    sentence-transformers/all-MiniLM-L12-v2:
      type: "sentence_transformer"
      dimensions: 384
      max_sequence_length: 256
      pooling: "mean"
      normalization: true
      device_preference: ["cpu", "cuda"]
      local_path: "./models/embeddings/all-MiniLM-L12-v2"
      use_local_files: true
      memory_requirements:
        gpu: "512MB"
        cpu: "256MB"

    BAAI/bge-m3:
      type: "sentence_transformer"
      dimensions: 1024
      max_sequence_length: 8192
      pooling: "cls"
      normalization: true
      multilingual: true
      device_preference: ["cuda"]
      memory_requirements:
        gpu: "4GB"
        cpu: "2GB"

    text-embedding-3-small:
      type: "openai"
      dimensions: 1536
      max_sequence_length: 8191
      api_based: true
      rate_limits:
        requests_per_minute: 3000
        tokens_per_minute: 1000000

# Reranking models
rerankers:
  models:
    cross-encoder/ms-marco-MiniLM-L-6-v2:
      type: "cross_encoder"
      max_sequence_length: 512
      score_range: [0, 1]
      device_preference: ["cuda", "cpu"]
      local_path: "./models/cross-encoders/ms-marco-MiniLM-L-6-v2"
      use_local_files: true
      memory_requirements:
        gpu: "1GB"
        cpu: "512MB"

    cross-encoder/ms-marco-MiniLM-L-12-v2:
      type: "cross_encoder"
      max_sequence_length: 512
      score_range: [0, 1]
      device_preference: ["cuda", "cpu"]
      local_path: "./models/cross-encoders/ms-marco-MiniLM-L-12-v2"
      use_local_files: true
      memory_requirements:
        gpu: "1GB"
        cpu: "512MB"

    BAAI/bge-reranker-v2-m3:
      type: "cross_encoder"
      max_sequence_length: 8192
      score_range: [-10, 10]
      normalization: "sigmoid"
      device_preference: ["cuda"]
      memory_requirements:
        gpu: "2GB"
        cpu: "1GB"

    rerank-english-v3.0:
      type: "cohere"
      max_sequence_length: 4096
      api_based: true
      rate_limits:
        requests_per_minute: 1000

# Graph embedding models
graph_embeddings:
  models:
    node2vec:
      dimensions: 128
      walk_length: 80
      num_walks: 10
      p: 1
      q: 1
      workers: 4

    graphsage:
      dimensions: 256
      num_layers: 2
      aggregator: "mean"
      dropout: 0.1

# Model compatibility matrix
compatibility:
  vector_stores:
    pgvector:
      supported_dimensions: [128, 256, 384, 512, 768, 1024, 1536, 2048]
      max_dimensions: 2048
      index_types: ["ivfflat", "hnsw"]

    qdrant:
      supported_dimensions: "any"
      max_dimensions: 65536
      distance_metrics: ["cosine", "euclidean", "dot"]

    weaviate:
      supported_dimensions: "any"
      max_dimensions: 65536
      distance_metrics: ["cosine", "dot", "l2-squared", "hamming", "manhattan"]

# Performance profiles
performance:
  embedding_benchmarks:
    intfloat/e5-large-v2:
      gpu_inference_time: "15ms"  # per text
      cpu_inference_time: "150ms"
      memory_usage_gpu: "2GB"
      memory_usage_cpu: "1GB"

    sentence-transformers/all-MiniLM-L12-v2:
      gpu_inference_time: "5ms"
      cpu_inference_time: "50ms"
      memory_usage_gpu: "512MB"
      memory_usage_cpu: "256MB"

  reranker_benchmarks:
    cross-encoder/ms-marco-MiniLM-L-12-v2:
      gpu_inference_time: "10ms"  # per pair
      cpu_inference_time: "100ms"
      batch_size_optimal: 16

# Model update policies
update_policies:
  automatic_updates: false
  security_updates: true
  version_pinning: true

  validation_requirements:
    performance_regression_threshold: 0.05
    accuracy_regression_threshold: 0.02
    latency_increase_threshold: 0.1

  rollback_triggers:
    error_rate_increase: 0.1
    latency_increase: 0.2
    accuracy_decrease: 0.05
